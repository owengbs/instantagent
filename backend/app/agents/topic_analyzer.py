"""
è¯é¢˜åˆ†æå™¨ - æ ¹æ®ç”¨æˆ·é—®é¢˜æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„é¦–å‘æ™ºèƒ½ä½“
"""

import re
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class AnalysisResult:
    """è¯é¢˜åˆ†æç»“æœ"""
    preferred_agent: Optional[str]  # æ¨èçš„é¦–å‘æ™ºèƒ½ä½“
    confidence: float               # ç½®ä¿¡åº¦ (0-1)
    matched_keywords: List[str]     # åŒ¹é…çš„å…³é”®è¯
    reason: str                     # é€‰æ‹©ç†ç”±
    scores: Dict[str, float]        # å„æ™ºèƒ½ä½“å¾—åˆ†è¯¦æƒ…

class TopicAnalyzer:
    """æ™ºèƒ½è¯é¢˜åˆ†æå™¨"""
    
    # æ™ºèƒ½ä½“ä¸“ä¸šé¢†åŸŸé…ç½®
    AGENT_EXPERTISE = {
        'buffett': {
            'keywords': [
                # ä»·å€¼æŠ•èµ„æ ¸å¿ƒæ¦‚å¿µ
                'ä»·å€¼æŠ•èµ„', 'é•¿æœŸæŠ•èµ„', 'åŸºæœ¬é¢åˆ†æ', 'è´¢åŠ¡åˆ†æ', 'ä¼ä¸šä»·å€¼', 'å†…åœ¨ä»·å€¼',
                'æŠ¤åŸæ²³', 'ç«äº‰ä¼˜åŠ¿', 'åˆ†çº¢', 'è‚¡æ¯', 'ç°é‡‘æµ', 'è‡ªç”±ç°é‡‘æµ',
                'å¸‚ç›ˆç‡', 'PE', 'å¸‚å‡€ç‡', 'PB', 'ROE', 'å‡€èµ„äº§æ”¶ç›Šç‡',
                
                # å·´è²ç‰¹ç›¸å…³
                'å·´è²ç‰¹', 'æ²ƒä¼¦', 'warren', 'buffett', 'ä¼¯å…‹å¸Œå°”', 'berkshire',
                'å¥¥é©¬å“ˆ', 'è‚¡ç¥', 'æŠ•èµ„ä¹‹ç¥',
                
                # æŠ•èµ„ç†å¿µ
                'é•¿æœŸæŒæœ‰', 'ä»·å€¼å‘ç°', 'å®‰å…¨è¾¹é™…', 'é€†å‘æŠ•èµ„', 'é›†ä¸­æŠ•èµ„',
                'èƒ½åŠ›åœˆ', 'æŠ¤åŸæ²³ä¼ä¸š', 'ä¼˜è´¨ä¼ä¸š', 'ç™½é©¬è‚¡',
                
                # ç›¸å…³å…¬å¸å’Œæ¡ˆä¾‹
                'å¯å£å¯ä¹', 'è‹¹æœ', 'apple', 'æ¯”äºšè¿ª', 'ä¸­çŸ³æ²¹', 'é“¶è¡Œè‚¡',
                
                # è‹±æ–‡å…³é”®è¯
                'value investing', 'long term', 'fundamental analysis', 'moat',
                'dividend', 'cash flow', 'intrinsic value', 'margin of safety'
            ],
            'strength_score': 1.0,
            'description': 'ä»·å€¼æŠ•èµ„å’Œé•¿æœŸæŒæœ‰ç­–ç•¥ä¸“å®¶'
        },
        
        'soros': {
            'keywords': [
                # å®è§‚æŠ•èµ„
                'å®è§‚æŠ•èµ„', 'å®è§‚ç»æµ', 'è´§å¸æ”¿ç­–', 'è´¢æ”¿æ”¿ç­–', 'æ±‡ç‡', 'å¤–æ±‡',
                'é‡åŒ–å®½æ¾', 'QE', 'åŠ æ¯', 'é™æ¯', 'é€šèƒ€', 'é€šç¼©',
                'ç»æµå‘¨æœŸ', 'ç»æµå±æœº', 'é‡‘èå±æœº', 'æ³¡æ²«', 'å´©ç›˜',
                
                # ç´¢ç½—æ–¯ç›¸å…³
                'ç´¢ç½—æ–¯', 'ä¹”æ²»', 'george', 'soros', 'é‡å­åŸºé‡‘', 'quantum',
                'é‡‘èå·¨é³„', 'åšç©ºä¹‹ç‹', 'åèº«æ€§ç†è®º',
                
                # æŠ•æœºå’Œå¯¹å†²
                'æŠ•æœº', 'å¯¹å†²', 'å¥—åˆ©', 'åšç©º', 'æ æ†', 'è¡ç”Ÿå“',
                'æœŸè´§', 'æœŸæƒ', 'å¤–æ±‡äº¤æ˜“', 'forex',
                
                # å®è§‚äº‹ä»¶
                'é»‘å¤©é¹…', 'ç°çŠ€ç‰›', 'åœ°ç¼˜æ”¿æ²»', 'è´¸æ˜“æˆ˜', 'è„±æ¬§',
                'ç¾è”å‚¨', 'fed', 'å¤®è¡Œ', 'æ¬§æ´²å¤®è¡Œ', 'æ—¥æœ¬å¤®è¡Œ',
                
                # å¸‚åœºæƒ…ç»ª
                'å¸‚åœºæƒ…ç»ª', 'ææ…Œ', 'è´ªå©ª', 'éç†æ€§', 'ç¾Šç¾¤æ•ˆåº”',
                'æŠ€æœ¯åˆ†æ', 'è¶‹åŠ¿', 'æ”¯æ’‘', 'é˜»åŠ›',
                
                # è‹±æ–‡å…³é”®è¯
                'macro investing', 'currency', 'forex', 'hedge fund',
                'speculation',                 'reflexivity', 'market sentiment', 'bubble'
            ],
            'strength_score': 1.0,
            'description': 'å®è§‚ç»æµåˆ†æå’ŒæŠ•æœºç­–ç•¥ä¸“å®¶'
        },
        
        'munger': {
            'keywords': [
                # å¤šå…ƒæ€ç»´æ¨¡å‹
                'å¤šå…ƒæ€ç»´', 'æ€ç»´æ¨¡å‹', 'æ ¼æ …ç†è®º', 'è·¨å­¦ç§‘', 'é€†å‘æ€è€ƒ', 'é€†å‘æ€ç»´',
                'è®¤çŸ¥åå·®', 'å¿ƒç†å­¦', 'è¡Œä¸ºé‡‘èå­¦', 'è®¤çŸ¥é™·é˜±', 'æ€ç»´é™·é˜±',
                
                # èŠ’æ ¼ç›¸å…³
                'èŠ’æ ¼', 'æŸ¥ç†', 'charlie', 'munger', 'å¤šå…ƒæ™ºæ…§', 'æ ¼æ …',
                
                # å­¦ä¹ å’Œæ™ºæ…§
                'ç»ˆèº«å­¦ä¹ ', 'å­¦ä¹ æœºå™¨', 'æ™ºæ…§', 'å¸¸è¯†', 'ç®€å•', 'å¤æ‚é—®é¢˜ç®€å•åŒ–',
                'ç¬¬ä¸€æ€§åŸç†', 'åŸºæœ¬åŸç†', 'æœ¬è´¨æ€è€ƒ',
                
                # å†³ç­–å’Œåˆ¤æ–­
                'å†³ç­–', 'åˆ¤æ–­', 'é€‰æ‹©', 'é”™è¯¯', 'å¤±è´¥', 'ç»éªŒæ•™è®­',
                'æ¦‚ç‡æ€ç»´', 'ç»Ÿè®¡æ€ç»´', 'æ•°å­¦æ€ç»´',
                
                # è·¨å­¦ç§‘æ¦‚å¿µ
                'ç‰©ç†å­¦', 'ç”Ÿç‰©å­¦', 'åŒ–å­¦', 'å·¥ç¨‹å­¦', 'ç³»ç»Ÿæ€ç»´',
                'å¤åˆæ•ˆåº”', 'ä¸´ç•Œç‚¹', 'ç½‘ç»œæ•ˆåº”', 'è§„æ¨¡æ•ˆåº”',
                
                # è‹±æ–‡å…³é”®è¯
                'mental models', 'multidisciplinary', 'cognitive bias',
                'invert', 'latticework', 'first principles'
            ],
            'strength_score': 1.0,
            'description': 'å¤šå…ƒæ€ç»´æ¨¡å‹å’Œè·¨å­¦ç§‘åˆ†æä¸“å®¶'
        }
    }
    
    # æƒé‡é…ç½®
    CONFIDENCE_THRESHOLD = 0.6  # ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼åˆ™éšæœºé€‰æ‹©
    KEYWORD_MATCH_WEIGHT = 1.0   # å…³é”®è¯åŒ¹é…æƒé‡
    CONTEXT_BOOST_WEIGHT = 0.2   # ä¸Šä¸‹æ–‡å¢å¼ºæƒé‡
    
    def __init__(self):
        """åˆå§‹åŒ–è¯é¢˜åˆ†æå™¨"""
        self._compile_keyword_patterns()
        logger.info("ğŸ§  è¯é¢˜åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _compile_keyword_patterns(self):
        """ç¼–è¯‘å…³é”®è¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œæé«˜åŒ¹é…æ•ˆç‡"""
        self.keyword_patterns = {}
        
        for agent_id, config in self.AGENT_EXPERTISE.items():
            patterns = []
            for keyword in config['keywords']:
                # æ”¯æŒä¸­è‹±æ–‡å…³é”®è¯ï¼Œå¿½ç•¥å¤§å°å†™
                pattern = re.escape(keyword)
                patterns.append(pattern)
            
            # ç»„åˆæˆå•ä¸ªæ­£åˆ™è¡¨è¾¾å¼
            combined_pattern = '|'.join(patterns)
            self.keyword_patterns[agent_id] = re.compile(
                combined_pattern, 
                re.IGNORECASE | re.UNICODE
            )
            
        logger.debug(f"ğŸ“ ç¼–è¯‘å…³é”®è¯æ¨¡å¼å®Œæˆ: {list(self.keyword_patterns.keys())}")
    
    def analyze_topic_preference(self, user_message: str, context: Optional[Dict] = None) -> AnalysisResult:
        """
        åˆ†æç”¨æˆ·é—®é¢˜çš„è¯é¢˜å€¾å‘æ€§
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚ä¼šè¯å†å²ï¼‰
            
        Returns:
            AnalysisResult: åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æè¯é¢˜å€¾å‘: '{user_message[:50]}...'")
            
            # é¢„å¤„ç†ç”¨æˆ·æ¶ˆæ¯
            processed_message = self._preprocess_message(user_message)
            
            # è®¡ç®—å„æ™ºèƒ½ä½“çš„åŒ¹é…å¾—åˆ†
            agent_scores = {}
            all_matched_keywords = {}
            
            for agent_id, pattern in self.keyword_patterns.items():
                matches = pattern.findall(processed_message)
                matched_keywords = list(set(matches))  # å»é‡
                
                # åŸºç¡€å…³é”®è¯åŒ¹é…å¾—åˆ†
                keyword_score = len(matched_keywords) * self.KEYWORD_MATCH_WEIGHT
                
                # ä¸Šä¸‹æ–‡å¢å¼ºå¾—åˆ†ï¼ˆå¦‚æœæä¾›ï¼‰
                context_score = self._calculate_context_score(agent_id, context) if context else 0
                
                # æ€»å¾—åˆ†
                total_score = keyword_score + context_score
                
                agent_scores[agent_id] = total_score
                all_matched_keywords[agent_id] = matched_keywords
                
                logger.debug(f"ğŸ“Š {agent_id} å¾—åˆ†: å…³é”®è¯={keyword_score}, ä¸Šä¸‹æ–‡={context_score}, æ€»åˆ†={total_score}")
                logger.debug(f"ğŸ¯ {agent_id} åŒ¹é…å…³é”®è¯: {matched_keywords}")
            
            # ç¡®å®šæ¨èçš„æ™ºèƒ½ä½“
            if not any(agent_scores.values()):
                # æ²¡æœ‰ä»»ä½•åŒ¹é…ï¼Œéšæœºé€‰æ‹©
                return AnalysisResult(
                    preferred_agent=None,
                    confidence=0.0,
                    matched_keywords=[],
                    reason="æœªæ‰¾åˆ°æ˜ç¡®çš„è¯é¢˜å€¾å‘ï¼Œå»ºè®®éšæœºé€‰æ‹©",
                    scores=agent_scores
                )
            
            # æ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„æ™ºèƒ½ä½“
            best_agent = max(agent_scores.items(), key=lambda x: x[1])
            best_agent_id, best_score = best_agent
            
            # è®¡ç®—ç½®ä¿¡åº¦
            total_score = sum(agent_scores.values())
            confidence = best_score / total_score if total_score > 0 else 0
            
            # ç”Ÿæˆé€‰æ‹©ç†ç”±
            reason = self._generate_reason(
                best_agent_id, 
                all_matched_keywords[best_agent_id], 
                confidence
            )
            
            result = AnalysisResult(
                preferred_agent=best_agent_id if confidence >= self.CONFIDENCE_THRESHOLD else None,
                confidence=confidence,
                matched_keywords=all_matched_keywords[best_agent_id],
                reason=reason,
                scores=agent_scores
            )
            
            logger.info(f"âœ… è¯é¢˜åˆ†æå®Œæˆ: æ¨è={result.preferred_agent}, ç½®ä¿¡åº¦={result.confidence:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ è¯é¢˜åˆ†æå¤±è´¥: {e}")
            return AnalysisResult(
                preferred_agent=None,
                confidence=0.0,
                matched_keywords=[],
                reason=f"åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}",
                scores={}
            )
    
    def _preprocess_message(self, message: str) -> str:
        """é¢„å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        # è½¬æ¢ä¸ºå°å†™ï¼Œä¾¿äºåŒ¹é…
        processed = message.lower()
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        processed = re.sub(r'\s+', ' ', processed).strip()
        
        return processed
    
    def _calculate_context_score(self, agent_id: str, context: Dict) -> float:
        """è®¡ç®—ä¸Šä¸‹æ–‡å¢å¼ºå¾—åˆ†"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä¼šè¯å†å²ã€ç”¨æˆ·åå¥½ç­‰è®¡ç®—é¢å¤–å¾—åˆ†
        # æš‚æ—¶è¿”å›0ï¼Œåç»­å¯ä»¥æ‰©å±•
        return 0.0
    
    def _generate_reason(self, agent_id: str, matched_keywords: List[str], confidence: float) -> str:
        """ç”Ÿæˆé€‰æ‹©ç†ç”±"""
        if not matched_keywords:
            return "æ— æ˜æ˜¾è¯é¢˜å€¾å‘"
        
        agent_desc = self.AGENT_EXPERTISE[agent_id]['description']
        keywords_str = "ã€".join(matched_keywords[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªå…³é”®è¯
        
        if len(matched_keywords) > 3:
            keywords_str += f"ç­‰{len(matched_keywords)}ä¸ªå…³é”®è¯"
        
        if confidence >= 0.8:
            confidence_desc = "å¼ºçƒˆ"
        elif confidence >= 0.6:
            confidence_desc = "è¾ƒå¼º"
        else:
            confidence_desc = "è½»å¾®"
        
        return f"æ£€æµ‹åˆ°{confidence_desc}çš„{agent_desc}å€¾å‘ï¼ŒåŒ¹é…å…³é”®è¯ï¼š{keywords_str}"
    
    def get_agent_expertise_summary(self) -> Dict[str, str]:
        """è·å–æ™ºèƒ½ä½“ä¸“ä¸šé¢†åŸŸæ‘˜è¦"""
        summary = {}
        for agent_id, config in self.AGENT_EXPERTISE.items():
            summary[agent_id] = {
                'description': config['description'],
                'keywords_count': len(config['keywords'])
            }
        return summary

# åˆ›å»ºå…¨å±€å®ä¾‹
topic_analyzer = TopicAnalyzer()