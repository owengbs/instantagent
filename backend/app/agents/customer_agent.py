"""
åŸºäºLangGraphçš„æ™ºèƒ½å®¢æœAgent
"""
import asyncio
import logging
import json
from typing import Dict, List, Any, Optional, Annotated
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel

from ..core.config import settings
from ..knowledge.knowledge_base import knowledge_base

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logger = logging.getLogger(__name__)


class ConversationState(BaseModel):
    """å¯¹è¯çŠ¶æ€æ•°æ®ç»“æ„"""
    messages: Annotated[List[Any], add_messages]
    user_id: str = "default"
    session_id: str = "default"
    context: Dict[str, Any] = {}
    retrieved_docs: List[Dict[str, Any]] = []
    last_query: str = ""
    turn_count: int = 0


class CustomerServiceAgent:
    """æ™ºèƒ½å®¢æœAgent"""
    
    def __init__(self):
        self.llm = None
        self.graph = None
        self.checkpointer = InMemorySaver()
        self.knowledge_retrieval_threshold = 0.5
        
    async def initialize(self):
        """åˆå§‹åŒ–Agent"""
        try:
            # åˆå§‹åŒ–å¤§æ¨¡å‹ - o4-miniåªæ”¯æŒåŸºæœ¬å‚æ•°
            self.llm = ChatOpenAI(
                base_url=settings.qwen.api_base,
                api_key=settings.qwen.api_key,
                model=settings.qwen.model,
                temperature=settings.qwen.temperature,
                max_tokens=settings.qwen.max_tokens,
                # ä½¿ç”¨Qwenæ¨¡å‹ï¼Œæ”¯æŒå®Œæ•´çš„OpenAIå…¼å®¹å‚æ•°
            )
            
            # æ„å»ºå¯¹è¯æµç¨‹å›¾
            await self._build_conversation_graph()
            
            print("å®¢æœAgentåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"å®¢æœAgentåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _build_conversation_graph(self):
        """æ„å»ºLangGraphå¯¹è¯æµç¨‹"""
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(ConversationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("retrieve_knowledge", self._retrieve_knowledge_node)
        workflow.add_node("generate_response", self._generate_response_node)
        workflow.add_node("update_context", self._update_context_node)
        
        # æ·»åŠ è¾¹ï¼ˆå®šä¹‰æµç¨‹ï¼‰
        workflow.add_edge(START, "retrieve_knowledge")
        workflow.add_edge("retrieve_knowledge", "generate_response")
        workflow.add_edge("generate_response", "update_context")
        workflow.add_edge("update_context", END)
        
        # ç¼–è¯‘å›¾
        self.graph = workflow.compile(checkpointer=self.checkpointer)
        
    async def _retrieve_knowledge_node(self, state: ConversationState) -> Dict[str, Any]:
        """çŸ¥è¯†æ£€ç´¢èŠ‚ç‚¹"""
        try:
            # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
            if not state.messages:
                return {"retrieved_docs": []}
            
            latest_message = state.messages[-1]
            if hasattr(latest_message, 'content'):
                query = latest_message.content
            else:
                query = str(latest_message)
            
            # ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = await knowledge_base.search(query, top_k=3)
            
            # è¿‡æ»¤ä½ç›¸å…³æ€§çš„æ–‡æ¡£
            filtered_docs = [
                doc for doc in retrieved_docs 
                if doc.get("score", 0) > self.knowledge_retrieval_threshold
            ]
            
            return {
                "retrieved_docs": filtered_docs,
                "last_query": query
            }
            
        except Exception as e:
            print(f"çŸ¥è¯†æ£€ç´¢å¤±è´¥: {e}")
            return {"retrieved_docs": []}
    
    async def _generate_response_node(self, state: ConversationState) -> Dict[str, Any]:
        """ç”Ÿæˆå›å¤èŠ‚ç‚¹"""
        try:
            logger.info(f"ğŸ§  å¼€å§‹ç”Ÿæˆå›å¤: turn_count={state.turn_count}")
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = ""
            if state.retrieved_docs:
                context_info = "\n".join([
                    f"ç›¸å…³ä¿¡æ¯ï¼š{doc['content']}"
                    for doc in state.retrieved_docs[:2]  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                ])
                logger.info(f"ğŸ“ æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯: {len(state.retrieved_docs)} ä¸ªæ–‡æ¡£, é•¿åº¦: {len(context_info)}")
            
            # æ„å»ºprompt
            system_prompt = self._build_system_prompt(context_info)
            logger.info(f"ğŸ’¬ ç³»ç»Ÿæç¤ºé•¿åº¦: {len(system_prompt)}")
            
            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = [SystemMessage(content=system_prompt)]
            
            # æ·»åŠ å†å²å¯¹è¯ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            recent_messages = state.messages[-6:] if len(state.messages) > 6 else state.messages
            messages.extend(recent_messages)
            
            logger.info(f"ğŸ“¨ å‡†å¤‡è°ƒç”¨LLM: æ¶ˆæ¯æ•°é‡={len(messages)}, æ¨¡å‹={settings.qwen.model}")
            logger.info(f"ğŸ”‘ APIé…ç½®: base_url={settings.qwen.api_base}, max_tokens={settings.qwen.max_tokens}")
            
            # è®°å½•å‘é€ç»™LLMçš„æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            if recent_messages:
                last_user_msg = recent_messages[-1]
                logger.info(f"ğŸ‘¤ ç”¨æˆ·æœ€æ–°æ¶ˆæ¯: '{last_user_msg.content if hasattr(last_user_msg, 'content') else str(last_user_msg)}'")
            
            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
            logger.info("ğŸš€ æ­£åœ¨è°ƒç”¨LLM...")
            response = await self.llm.ainvoke(messages)
            
            logger.info(f"ğŸ’¡ LLMå›å¤æˆåŠŸ: ç±»å‹={type(response).__name__}")
            if hasattr(response, 'content'):
                logger.info(f"ğŸ“ LLMå›å¤å†…å®¹: '{response.content[:200]}...' (æ€»é•¿åº¦: {len(response.content)})")
            
            return {
                "messages": [response],
                "turn_count": state.turn_count + 1
            }
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {type(e).__name__}: {str(e)}", exc_info=True)
            # è¿”å›å‹å¥½çš„é”™è¯¯å›å¤
            error_response = AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ï¼Œæˆ–è”ç³»äººå·¥å®¢æœå¯»æ±‚å¸®åŠ©ã€‚")
            return {
                "messages": [error_response],
                "turn_count": state.turn_count + 1
            }
    
    async def _update_context_node(self, state: ConversationState) -> Dict[str, Any]:
        """æ›´æ–°ä¸Šä¸‹æ–‡èŠ‚ç‚¹"""
        try:
            # æ›´æ–°ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯
            updated_context = state.context.copy()
            updated_context.update({
                "last_interaction": datetime.now().isoformat(),
                "total_turns": state.turn_count,
                "last_retrieved_categories": [
                    doc.get("metadata", {}).get("category", "")
                    for doc in state.retrieved_docs
                ]
            })
            
            return {"context": updated_context}
            
        except Exception as e:
            print(f"æ›´æ–°ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return {}
    
    def _build_system_prompt(self, context_info: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        base_prompt = settings.agent.system_prompt
        
        if context_info:
            knowledge_section = f"\n\nå‚è€ƒä¿¡æ¯ï¼š\n{context_info}\n\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸å¤Ÿå……åˆ†ï¼Œè¯·è¯šå®å‘ŠçŸ¥å¹¶å»ºè®®è”ç³»äººå·¥å®¢æœã€‚"
        else:
            knowledge_section = "\n\nå½“å‰æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†åº“ä¿¡æ¯ï¼Œè¯·æ ¹æ®ä½ çš„å¸¸è¯†å‹å¥½åœ°å›ç­”ï¼Œå¦‚æœä¸ç¡®å®šè¯·å»ºè®®ç”¨æˆ·è”ç³»äººå·¥å®¢æœã€‚"
        
        return base_prompt + knowledge_section
    
    async def chat(self, message: str, user_id: str = "default", session_id: str = "default") -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›å›å¤"""
        try:
            logger.info(f"ğŸ¤ æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: user_id={user_id}, session_id={session_id}, message='{message}'")
            
            # å‡†å¤‡è¾“å…¥çŠ¶æ€
            input_state = {
                "messages": [HumanMessage(content=message)],
                "user_id": user_id,
                "session_id": session_id,
            }
            
            # é…ç½®çº¿ç¨‹IDï¼ˆç”¨äºä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
            config = {
                "configurable": {
                    "thread_id": f"{user_id}_{session_id}"
                }
            }
            
            logger.info(f"ğŸ¤– å¼€å§‹è°ƒç”¨LangGraphå¤„ç†: thread_id={user_id}_{session_id}")
            
            # è¿è¡Œå¯¹è¯æµç¨‹
            result = await self.graph.ainvoke(input_state, config)
            
            logger.info(f"ğŸ“Š LangGraphå¤„ç†å®Œæˆ: result_keys={list(result.keys())}")
            
            # æå–å›å¤å†…å®¹
            if result.get("messages"):
                latest_message = result["messages"][-1]
                response_content = latest_message.content if hasattr(latest_message, 'content') else str(latest_message)
                logger.info(f"âœ… ç”Ÿæˆå›å¤: '{response_content[:100]}...' (é•¿åº¦: {len(response_content)})")
            else:
                response_content = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ï¼Œèƒ½å¦è¯·æ‚¨é‡æ–°æè¿°ä¸€ä¸‹ï¼Ÿ"
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå›å¤ï¼Œä½¿ç”¨é»˜è®¤å›å¤")
            
            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£
            retrieved_docs = result.get("retrieved_docs", [])
            if retrieved_docs:
                logger.info(f"ğŸ“š æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            response_data = {
                "response": response_content,
                "retrieved_docs": retrieved_docs,
                "turn_count": result.get("turn_count", 0),
                "context": result.get("context", {})
            }
            
            logger.info(f"ğŸ¯ è¿”å›å®Œæ•´å›å¤: turn_count={response_data['turn_count']}")
            return response_data
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¯¹è¯å¤±è´¥: {type(e).__name__}: {str(e)}", exc_info=True)
            return {
                "response": "æŠ±æ­‰ï¼Œç³»ç»Ÿé‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åå†è¯•æˆ–è”ç³»äººå·¥å®¢æœã€‚",
                "retrieved_docs": [],
                "turn_count": 0,
                "context": {}
            }
    
    async def get_conversation_history(self, user_id: str = "default", session_id: str = "default") -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        try:
            config = {
                "configurable": {
                    "thread_id": f"{user_id}_{session_id}"
                }
            }
            
            # è·å–æ£€æŸ¥ç‚¹çŠ¶æ€
            state = await self.graph.aget_state(config)
            
            if state and state.values.get("messages"):
                history = []
                for msg in state.values["messages"]:
                    if hasattr(msg, 'type') and hasattr(msg, 'content'):
                        history.append({
                            "type": msg.type,
                            "content": msg.content,
                            "timestamp": getattr(msg, 'timestamp', None)
                        })
                return history
            
            return []
            
        except Exception as e:
            print(f"è·å–å¯¹è¯å†å²å¤±è´¥: {e}")
            return []
    
    async def clear_conversation(self, user_id: str = "default", session_id: str = "default") -> bool:
        """æ¸…é™¤å¯¹è¯å†å²"""
        try:
            thread_id = f"{user_id}_{session_id}"
            # æ³¨æ„ï¼šInMemorySaveræ²¡æœ‰ç›´æ¥çš„åˆ é™¤æ–¹æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥é‡æ–°åˆå§‹åŒ–checkpointer
            # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥ä½¿ç”¨æ”¯æŒåˆ é™¤çš„æŒä¹…åŒ–checkpointer
            return True
            
        except Exception as e:
            print(f"æ¸…é™¤å¯¹è¯å†å²å¤±è´¥: {e}")
            return False
    
    async def chat_stream(self, message: str, user_id: str = "default", session_id: str = "default"):
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›æµå¼å›å¤ç”Ÿæˆå™¨"""
        try:
            logger.info(f"ğŸŒŠ æ”¶åˆ°æµå¼èŠå¤©è¯·æ±‚: user_id={user_id}, session_id={session_id}, message='{message}'")
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ - ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å¤æ‚çš„å›¾ç»“æ„
            context_info = ""
            
            # å¯é€‰ï¼šæ·»åŠ çŸ¥è¯†åº“æ£€ç´¢
            try:
                retrieved_docs = await knowledge_base.search(message, top_k=2)
                if retrieved_docs:
                    context_info = "\n".join([
                        f"ç›¸å…³ä¿¡æ¯ï¼š{doc['content']}"
                        for doc in retrieved_docs[:2]
                    ])
                    logger.info(f"ğŸ“š æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£: {len(retrieved_docs)} ä¸ª")
            except Exception as e:
                logger.warning(f"âš ï¸ çŸ¥è¯†åº“æ£€ç´¢å¤±è´¥: {e}")
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(context_info)
            
            # å‡†å¤‡æ¶ˆæ¯ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾èµ–å®Œæ•´çš„çŠ¶æ€ç®¡ç†ï¼‰
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=message)
            ]
            
            logger.info(f"ğŸš€ å¼€å§‹æµå¼LLMè°ƒç”¨...")
            
            # ä½¿ç”¨æµå¼è°ƒç”¨
            full_response = ""
            async for chunk in self.llm.astream(messages):
                if hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    full_response += content
                    logger.debug(f"ğŸ“ LLMæµå¼ç‰‡æ®µ: '{content}'")
                    yield content
            
            logger.info(f"âœ… æµå¼å›å¤å®Œæˆ: æ€»é•¿åº¦={len(full_response)}")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼èŠå¤©å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"

    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•å¤§æ¨¡å‹è¿æ¥
            test_response = await self.llm.ainvoke([
                SystemMessage(content="è¯·ç®€å•å›å¤'ç³»ç»Ÿæ­£å¸¸'")
            ])
            
            # æµ‹è¯•çŸ¥è¯†åº“è¿æ¥
            test_search = await knowledge_base.search("æµ‹è¯•", top_k=1)
            
            return {
                "status": "healthy",
                "llm_status": "connected",
                "knowledge_base_status": "connected",
                "knowledge_base_count": knowledge_base.collection.count() if knowledge_base.collection else 0,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


# å…¨å±€Agentå®ä¾‹
customer_agent = CustomerServiceAgent() 