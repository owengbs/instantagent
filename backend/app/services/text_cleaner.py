"""
æ–‡æœ¬æ¸…ç†æœåŠ¡
å¤„ç†å¤§æ¨¡å‹è¾“å‡ºï¼Œä½¿å…¶é€‚åˆTTSè¯­éŸ³åˆæˆ
"""
import re
import logging
from typing import List

logger = logging.getLogger(__name__)

class TextCleaner:
    """æ–‡æœ¬æ¸…ç†å™¨"""
    
    def __init__(self):
        # éœ€è¦ç§»é™¤çš„ç¬¦å·å’Œæ¨¡å¼
        self.remove_patterns = [
            # è¡¨æƒ…ç¬¦å·å’Œç‰¹æ®Šç¬¦å·
            r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]',  # Unicodeè¡¨æƒ…ç¬¦å·
            r'[ğŸ˜€-ğŸ˜¿]',  # æ›´å¤šè¡¨æƒ…ç¬¦å·
            r'[ğŸš€-ğŸ›¿]',  # äº¤é€šå·¥å…·è¡¨æƒ…
            r'[âš¡-âš¿]',  # ç¬¦å·è¡¨æƒ…
            r'[ğŸ’¡-ğŸ’¿]',  # ç‰©å“è¡¨æƒ…
            r'[ğŸ“±-ğŸ“¿]',  # è®¾å¤‡è¡¨æƒ…
            r'[ğŸµ-ğŸ¿]',  # å¨±ä¹è¡¨æƒ…
            r'[ğŸ€-ğŸ¿]',  # è¿åŠ¨è¡¨æƒ…
            r'[ğŸŒ-ğŸŒ¿]',  # è‡ªç„¶è¡¨æƒ…
            r'[ğŸ€-ğŸ¿]',  # é£Ÿç‰©è¡¨æƒ…
            
            # æ ‡é¢˜æ ¼å¼
            r'^#{1,6}\s+',  # Markdownæ ‡é¢˜
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€',  # ä¸­æ–‡æ•°å­—æ ‡é¢˜
            r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+',  # åœ†åœˆæ•°å­—
            r'^[â‘´â‘µâ‘¶â‘·â‘¸â‘¹â‘ºâ‘»â‘¼â‘½]+',  # æ‹¬å·æ•°å­—
            
            # åˆ—è¡¨æ ¼å¼
            r'^\d+\.\s+',  # æ•°å­—åˆ—è¡¨
            r'^[â€¢Â·]\s+',  # é¡¹ç›®ç¬¦å·
            r'^[-*]\s+',  # çŸ­æ¨ªçº¿åˆ—è¡¨
            
            # ç‰¹æ®Šç¬¦å·
            r'[ã€ã€‘]',  # ä¸­æ–‡æ–¹æ‹¬å·
            r'[ã€Œã€]',  # ä¸­æ–‡å¼•å·
            r'[ã€ã€]',  # ä¸­æ–‡ä¹¦åå·
            r'[ã€ˆã€‰]',  # ä¸­æ–‡å°–æ‹¬å·
            r'[ã€Šã€‹]',  # ä¸­æ–‡ä¹¦åå·
            r'[ï¼ˆï¼‰]',  # ä¸­æ–‡æ‹¬å·
            r'[ï¼»ï¼½]',  # ä¸­æ–‡æ–¹æ‹¬å·
            r'[ï½›ï½]',  # ä¸­æ–‡å¤§æ‹¬å·
            r'[ã€ã€‘]',  # ä¸­æ–‡æ–¹æ‹¬å·
            
            # å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
            r'[ï¼]{2,}',  # å¤šä¸ªæ„Ÿå¹å·
            r'[ï¼Ÿ]{2,}',  # å¤šä¸ªé—®å·
            r'[ã€‚]{2,}',  # å¤šä¸ªå¥å·
            r'[ï¼Œ]{2,}',  # å¤šä¸ªé€—å·
            r'[ï¼›]{2,}',  # å¤šä¸ªåˆ†å·
            r'[ï¼š]{2,}',  # å¤šä¸ªå†’å·
            
            # ç‰¹æ®Šå­—ç¬¦
            r'[~`!@#$%^&*()_+\-=\[\]{}|\\:";\'<>?,./]',  # è‹±æ–‡ç‰¹æ®Šç¬¦å·
            
            # å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
            r'\n+',  # å¤šä¸ªæ¢è¡Œç¬¦
            r'\s+',  # å¤šä¸ªç©ºæ ¼
        ]
        
        # éœ€è¦æ›¿æ¢çš„æ¨¡å¼
        self.replace_patterns = [
            # å°†æŸäº›ç¬¦å·æ›¿æ¢ä¸ºè‡ªç„¶è¯­è¨€
            (r'ï¼ˆ', 'ï¼Œ'),
            (r'ï¼‰', 'ï¼Œ'),
            (r'ã€', 'ï¼Œ'),
            (r'ã€‘', 'ï¼Œ'),
            (r'ã€Š', 'ï¼Œ'),
            (r'ã€‹', 'ï¼Œ'),
            (r'ã€Œ', 'ï¼Œ'),
            (r'ã€', 'ï¼Œ'),
            (r'ã€', 'ï¼Œ'),
            (r'ã€', 'ï¼Œ'),
            (r'ã€ˆ', 'ï¼Œ'),
            (r'ã€‰', 'ï¼Œ'),
            (r'ï¼»', 'ï¼Œ'),
            (r'ï¼½', 'ï¼Œ'),
            (r'ï½›', 'ï¼Œ'),
            (r'ï½', 'ï¼Œ'),
            
            # å°†åˆ—è¡¨æ ¼å¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
            (r'^\d+\.\s+', 'ç¬¬'),
            (r'^[â€¢Â·]\s+', ''),
            (r'^[-*]\s+', ''),
            
            # å°†æ ‡é¢˜æ ¼å¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€
            (r'^#{1,6}\s+', ''),
            (r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€', ''),
            (r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+', ''),
            (r'^[â‘´â‘µâ‘¶â‘·â‘¸â‘¹â‘ºâ‘»â‘¼â‘½]+', ''),
        ]
        
        # éœ€è¦ä¿ç•™çš„æ ‡ç‚¹ç¬¦å·ï¼ˆç”¨äºè¯­éŸ³åœé¡¿ï¼‰
        self.keep_punctuation = r'[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š]'
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬ï¼Œä½¿å…¶é€‚åˆTTSè¯­éŸ³åˆæˆ
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        logger.debug(f"ğŸ§¹ å¼€å§‹æ¸…ç†æ–‡æœ¬: åŸå§‹é•¿åº¦={len(text)}")
        
        # 1. ç§»é™¤ä¸éœ€è¦çš„ç¬¦å·å’Œæ ¼å¼
        cleaned_text = text
        for pattern in self.remove_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.MULTILINE)
        
        # 2. æ›¿æ¢æŸäº›ç¬¦å·ä¸ºè‡ªç„¶è¯­è¨€
        for pattern, replacement in self.replace_patterns:
            cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.MULTILINE)
        
        # 3. å¤„ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
        cleaned_text = re.sub(r'[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š]{2,}', lambda m: m.group()[0], cleaned_text)
        
        # 4. å¤„ç†å¤šä½™çš„ç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        # 5. å¤„ç†å¥å­è¿æ¥
        cleaned_text = self._improve_sentence_flow(cleaned_text)
        
        # 6. æœ€ç»ˆæ¸…ç†
        cleaned_text = cleaned_text.strip()
        
        logger.debug(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆ: æ¸…ç†åé•¿åº¦={len(cleaned_text)}")
        return cleaned_text
    
    def _improve_sentence_flow(self, text: str) -> str:
        """
        æ”¹å–„å¥å­æµç•…åº¦
        
        Args:
            text: å¾…å¤„ç†çš„æ–‡æœ¬
            
        Returns:
            str: æ”¹å–„åçš„æ–‡æœ¬
        """
        # å¤„ç†å¥å­ä¹‹é—´çš„è¿æ¥
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])', r'\1 ç¬¬\2', text)
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©])', r'\1 ç¬¬\2', text)
        
        # å¤„ç†æ•°å­—åˆ—è¡¨
        text = re.sub(r'(\d+)\s*[ã€ï¼Œ]\s*', r'ç¬¬\1ä¸ªï¼Œ', text)
        
        # å¤„ç†é¡¹ç›®ç¬¦å·
        text = re.sub(r'[â€¢Â·]\s*', 'ï¼Œ', text)
        
        # ç¡®ä¿å¥å­ä»¥åˆé€‚çš„æ ‡ç‚¹ç»“å°¾
        if text and not re.search(r'[ã€‚ï¼ï¼Ÿ]$', text):
            text += 'ã€‚'
        
        return text
    
    def clean_for_tts(self, text: str) -> str:
        """
        ä¸“é—¨ä¸ºTTSä¼˜åŒ–çš„æ–‡æœ¬æ¸…ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: é€‚åˆTTSçš„æ–‡æœ¬
        """
        # åŸºç¡€æ¸…ç†
        cleaned = self.clean_text(text)
        
        # TTSç‰¹å®šä¼˜åŒ–
        cleaned = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([ã€‚ï¼ï¼Ÿ])', r'\1', cleaned)  # ç§»é™¤é‡å¤æ ‡ç‚¹
        cleaned = re.sub(r'([ï¼Œï¼›ï¼š])\s*([ï¼Œï¼›ï¼š])', r'\1', cleaned)  # ç§»é™¤é‡å¤åˆ†éš”ç¬¦
        
        # ç¡®ä¿æ–‡æœ¬ä¸ä¸ºç©º
        if not cleaned.strip():
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·é‡æ–°æè¿°ä¸€ä¸‹ã€‚"
        
        return cleaned
    
    def split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        # æŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·åˆ†å‰²
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        
        # é‡ç»„å¥å­
        result = []
        current_sentence = ""
        
        for i, part in enumerate(sentences):
            if part in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                current_sentence += part
                if current_sentence.strip():
                    result.append(current_sentence.strip())
                current_sentence = ""
            else:
                current_sentence += part
        
        # å¤„ç†æœ€åä¸€ä¸ªå¥å­
        if current_sentence.strip():
            result.append(current_sentence.strip())
        
        return [s for s in result if s.strip()]

# å…¨å±€æ–‡æœ¬æ¸…ç†å™¨å®ä¾‹
text_cleaner = TextCleaner() 