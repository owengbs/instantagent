"""
Qwenå®æ—¶è¯­éŸ³è¯†åˆ«æœåŠ¡
åŸºäºé˜¿é‡Œäº‘ç™¾ç‚¼çš„å®æ—¶è¯­éŸ³è¯†åˆ«API
ä½¿ç”¨å®˜æ–¹dashscope SDK
å‚è€ƒå®˜æ–¹ç¤ºä¾‹ï¼šhttps://github.com/aliyun/alibabacloud-bailian-speech-demo/blob/master/samples/speech-recognition/recognize_speech_from_microphone/python/run.py
"""
import asyncio
import logging
import json
from typing import Optional, Callable, Dict, Any, AsyncGenerator
from datetime import datetime
from ..utils.logging_decorator import log_api_call
import time
import numpy as np

# å¯¼å…¥dashscope SDK
try:
    import dashscope
    from dashscope.audio.asr import Recognition, RecognitionCallback, RecognitionResult
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    logging.warning("dashscope SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install dashscope")

logger = logging.getLogger(__name__)

def resample_audio(audio_data: bytes, from_rate: int, to_rate: int) -> bytes:
    """
    ç®€å•çš„éŸ³é¢‘é‡é‡‡æ ·å‡½æ•°
    å°†éŸ³é¢‘ä»from_rateé‡‡æ ·ç‡è½¬æ¢ä¸ºto_rateé‡‡æ ·ç‡
    
    Args:
        audio_data: åŸå§‹éŸ³é¢‘æ•°æ®ï¼ˆå­—èŠ‚ï¼‰
        from_rate: åŸå§‹é‡‡æ ·ç‡
        to_rate: ç›®æ ‡é‡‡æ ·ç‡
        
    Returns:
        bytes: é‡é‡‡æ ·åçš„éŸ³é¢‘æ•°æ®
    """
    try:
        # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        
        # è®¡ç®—é‡é‡‡æ ·æ¯”ä¾‹
        ratio = to_rate / from_rate
        
        # ç®€å•çš„çº¿æ€§æ’å€¼é‡é‡‡æ ·
        new_length = int(len(audio_array) * ratio)
        resampled = np.interp(
            np.linspace(0, len(audio_array), new_length),
            np.arange(len(audio_array)),
            audio_array
        )
        
        # è½¬æ¢å›int16å¹¶è¿”å›å­—èŠ‚
        return resampled.astype(np.int16).tobytes()
        
    except Exception as e:
        logger.warning(f"âš ï¸ é‡é‡‡æ ·å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return audio_data

class QwenASRCallback(RecognitionCallback):
    """Qwen ASRå›è°ƒå¤„ç†ç±»"""
    
    def __init__(self, on_result: Optional[Callable[[str, bool], None]] = None):
        self.on_result = on_result
        self.is_open = False
        self.last_partial_text = ""
        self.silence_start_time = None
        self.silence_threshold = 2.0  # 2ç§’é™éŸ³åè®¤ä¸ºè¯´è¯ç»“æŸ
        self.last_activity_time = time.time()
        
    def on_open(self) -> None:
        """è¿æ¥æ‰“å¼€å›è°ƒ"""
        logger.info("ğŸ¤ Qwen ASRè¿æ¥å·²æ‰“å¼€")
        self.is_open = True
        self.last_activity_time = time.time()
        
    def on_close(self) -> None:
        """è¿æ¥å…³é—­å›è°ƒ"""
        logger.info("ğŸ¤ Qwen ASRè¿æ¥å·²å…³é—­")
        self.is_open = False
        
    def on_event(self, result: RecognitionResult) -> None:
        """è¯†åˆ«ç»“æœå›è°ƒ"""
        try:
            # æ›´æ–°æ´»åŠ¨æ—¶é—´
            self.last_activity_time = time.time()
            
            # è·å–è¯†åˆ«ç»“æœ
            sentence_data = result.get_sentence()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯†åˆ«ç»“æœ
            if sentence_data:
                # æå–æ–‡æœ¬å†…å®¹
                if isinstance(sentence_data, dict):
                    text = sentence_data.get('text', '')
                elif isinstance(sentence_data, list) and len(sentence_data) > 0:
                    text = sentence_data[0].get('text', '') if isinstance(sentence_data[0], dict) else str(sentence_data[0])
                else:
                    text = str(sentence_data)
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯å¥å­ç»“æŸ
                is_final = RecognitionResult.is_sentence_end(sentence_data)
                
                if text and text.strip():
                    if is_final:
                        # å¥å­çº§åˆ«ç»“æœ
                        sentence_text = text.strip()
                        logger.info(f"ğŸ¤ Qwen ASRå¥å­ç»“æœ: '{sentence_text}'")
                        if self.on_result:
                            self.on_result(sentence_text, True)  # å¥å­çº§åˆ«ç»“æœ
                        # é‡ç½®é™éŸ³è®¡æ—¶å™¨
                        self.silence_start_time = None
                        self.last_partial_text = ""
                    else:
                        # éƒ¨åˆ†è¯†åˆ«ç»“æœ
                        partial_text = text.strip()
                        if partial_text != self.last_partial_text:
                            logger.info(f"ğŸ¤ Qwen ASRéƒ¨åˆ†ç»“æœ: '{partial_text}'")
                            self.last_partial_text = partial_text
                            if self.on_result:
                                self.on_result(partial_text, False)  # éƒ¨åˆ†ç»“æœ
                            # é‡ç½®é™éŸ³è®¡æ—¶å™¨
                            self.silence_start_time = None
            else:
                # æ²¡æœ‰è¯†åˆ«ç»“æœï¼Œå¯èƒ½æ˜¯é™éŸ³
                current_time = time.time()
                if self.silence_start_time is None:
                    self.silence_start_time = current_time
                elif current_time - self.silence_start_time > self.silence_threshold:
                    # é™éŸ³è¶…è¿‡é˜ˆå€¼ï¼Œå¯ä»¥è®¤ä¸ºè¯´è¯ç»“æŸ
                    if self.last_partial_text:
                        logger.info(f"ğŸ¤ æ£€æµ‹åˆ°é™éŸ³ï¼Œæœ€ç»ˆç»“æœ: '{self.last_partial_text}'")
                        if self.on_result:
                            self.on_result(self.last_partial_text, True)
                        self.last_partial_text = ""
                    
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ASRç»“æœå¤±è´¥: {e}")
            # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

class QwenASRRealtimeService:
    """
    Qwenå®æ—¶è¯­éŸ³è¯†åˆ«æœåŠ¡
    ä½¿ç”¨å®˜æ–¹dashscope SDK
    """
    
    def __init__(self):
        self.api_key = "sk-ff980442223b45868202e5cb35724bb1"
        self.default_model = "paraformer-realtime-v2"
        self.sample_rate = 16000
        self.channels = 1
        self.format = "pcm"
        
        # è®¾ç½®API Key
        if DASHSCOPE_AVAILABLE:
            dashscope.api_key = self.api_key
            logger.info("âœ… dashscope SDKå·²é…ç½®")
        else:
            logger.error("âŒ dashscope SDKæœªå®‰è£…")
    
    @log_api_call("Qwen ASR Realtime", "qwen")
    async def recognize_stream(
        self, 
        audio_stream: AsyncGenerator[bytes, None],
        model: str = None,
        on_result: Optional[Callable[[str, bool], None]] = None,
        input_sample_rate: int = 16000,  # è¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡
        language: str = "zh-CN"  # æ·»åŠ è¯­è¨€å‚æ•°
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼è¯­éŸ³è¯†åˆ«
        
        Args:
            audio_stream: éŸ³é¢‘æµç”Ÿæˆå™¨
            model: è¯†åˆ«æ¨¡å‹ï¼Œé»˜è®¤paraformer-realtime-v2
            on_result: ç»“æœå›è°ƒå‡½æ•° (text, is_final)
            input_sample_rate: è¾“å…¥éŸ³é¢‘çš„é‡‡æ ·ç‡ï¼Œé»˜è®¤16kHz
            language: è¯†åˆ«è¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡
            
        Yields:
            str: è¯†åˆ«å‡ºçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if not DASHSCOPE_AVAILABLE:
            raise Exception("dashscope SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install dashscope")
            
        if not model:
            model = self.default_model
            
        logger.info(f"ğŸ¤ å¼€å§‹å®æ—¶è¯­éŸ³è¯†åˆ«: model={model}, input_sample_rate={input_sample_rate}, language={language}")
        
        # ç”¨äºæ”¶é›†ç»“æœçš„åˆ—è¡¨
        results = []
        
        def result_callback(text: str, is_final: bool):
            if text.strip():
                if is_final:
                    results.append(text)
                if on_result:
                    on_result(text, is_final)
        
        try:
            # åˆ›å»ºå›è°ƒå¤„ç†å™¨
            callback = QwenASRCallback(result_callback)
            
            # åˆ›å»ºè¯†åˆ«å®ä¾‹ï¼Œæ·»åŠ è¯­è¨€å‚æ•°
            recognition = Recognition(
                model=model,
                format=self.format,
                sample_rate=self.sample_rate,
                callback=callback,
                # æ·»åŠ è¯­è¨€å‚æ•°
                language=language if language else "zh-CN"
            )
            
            logger.info("ğŸš€ å¯åŠ¨Qwen ASRè¯†åˆ«...")
            recognition.start()
            
            # ç­‰å¾…è¿æ¥å»ºç«‹
            await asyncio.sleep(1.0)  # å¢åŠ ç­‰å¾…æ—¶é—´
            
            if not callback.is_open:
                raise Exception("ASRè¿æ¥å»ºç«‹å¤±è´¥")
            
            logger.info("âœ… Qwen ASRè¿æ¥å·²å»ºç«‹ï¼Œå¼€å§‹å¤„ç†éŸ³é¢‘æµ")
            
            # å¤„ç†éŸ³é¢‘æµ
            chunk_count = 0
            last_activity_time = time.time()
            
            async for audio_chunk in audio_stream:
                try:
                    # æ£€æŸ¥è¿æ¥çŠ¶æ€
                    if not callback.is_open:
                        logger.warning("âš ï¸ ASRè¿æ¥å·²å…³é—­ï¼Œåœæ­¢å¤„ç†")
                        break
                    
                    # æ£€æŸ¥éŸ³é¢‘æ•°æ®æ ¼å¼
                    if len(audio_chunk) % 2 != 0:
                        logger.warning(f"âš ï¸ éŸ³é¢‘æ•°æ®é•¿åº¦ä¸æ˜¯2çš„å€æ•°: {len(audio_chunk)}")
                        continue
                    
                    # å¦‚æœè¾“å…¥é‡‡æ ·ç‡ä¸ASRæœŸæœ›çš„é‡‡æ ·ç‡ä¸åŒï¼Œè¿›è¡Œé‡é‡‡æ ·
                    processed_chunk = audio_chunk
                    if input_sample_rate != self.sample_rate:
                        processed_chunk = resample_audio(audio_chunk, input_sample_rate, self.sample_rate)
                        logger.debug(f"ğŸ”„ é‡é‡‡æ ·: {input_sample_rate}Hz -> {self.sample_rate}Hz, {len(audio_chunk)} -> {len(processed_chunk)} bytes")
                    
                    # å‘é€éŸ³é¢‘å¸§
                    recognition.send_audio_frame(processed_chunk)
                    chunk_count += 1
                    last_activity_time = time.time()
                    
                    # æ¯20ä¸ªå—è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼Œå‡å°‘æ—¥å¿—é‡
                    if chunk_count % 20 == 0:
                        logger.info(f"ğŸ¤ å·²å¤„ç†éŸ³é¢‘å—: {chunk_count}")
                    
                    # æ£€æŸ¥æ´»åŠ¨è¶…æ—¶ï¼ˆ30ç§’æ— æ´»åŠ¨è‡ªåŠ¨åœæ­¢ï¼‰
                    if time.time() - last_activity_time > 30:
                        logger.warning("âš ï¸ æ´»åŠ¨è¶…æ—¶ï¼Œåœæ­¢å¤„ç†")
                        break
                        
                except Exception as e:
                    logger.error(f"âŒ å‘é€éŸ³é¢‘å¸§å¤±è´¥: {e}")
                    # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œåœæ­¢å¤„ç†
                    if "closing transport" in str(e) or "Speech recognition has stopped" in str(e):
                        logger.warning("âš ï¸ æ£€æµ‹åˆ°è¿æ¥é”™è¯¯ï¼Œåœæ­¢å¤„ç†")
                        break
                    continue
            
            logger.info(f"ğŸ“¤ éŸ³é¢‘æµå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªéŸ³é¢‘å—")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æœ€åçš„è¯†åˆ«ç»“æœå¤„ç†å®Œæˆ
            await asyncio.sleep(1.0)
            
            # åœæ­¢è¯†åˆ«
            try:
                recognition.stop()
                logger.info("âœ… Qwen ASRè¯†åˆ«å·²åœæ­¢")
            except Exception as e:
                logger.warning(f"âš ï¸ åœæ­¢è¯†åˆ«æ—¶å‡ºé”™: {e}")
            
            # è¿”å›æ‰€æœ‰ç»“æœ
            for result in results:
                yield result
                
        except Exception as e:
            logger.error(f"âŒ å®æ—¶è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            raise
    
    @log_api_call("Qwen ASR File", "qwen")
    async def recognize_file(self, file_path: str, model: str = None) -> str:
        """
        è¯†åˆ«éŸ³é¢‘æ–‡ä»¶
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            model: è¯†åˆ«æ¨¡å‹
            
        Returns:
            str: è¯†åˆ«ç»“æœ
        """
        if not DASHSCOPE_AVAILABLE:
            raise Exception("dashscope SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install dashscope")
            
        if not model:
            model = self.default_model
            
        logger.info(f"ğŸ¤ å¼€å§‹æ–‡ä»¶è¯­éŸ³è¯†åˆ«: file={file_path}, model={model}")
        
        try:
            # è¯»å–éŸ³é¢‘æ–‡ä»¶
            with open(file_path, 'rb') as f:
                audio_data = f.read()
            
            # åˆ›å»ºå›è°ƒå¤„ç†å™¨
            results = []
            def on_result(text: str, is_final: bool):
                if is_final:
                    results.append(text)
            
            callback = QwenASRCallback(on_result)
            
            # åˆ›å»ºè¯†åˆ«å®ä¾‹
            recognition = Recognition(
                model=model,
                format=self.format,
                sample_rate=self.sample_rate,
                callback=callback
            )
            
            logger.info("ğŸš€ å¯åŠ¨æ–‡ä»¶è¯†åˆ«...")
            recognition.start()
            
            # ç­‰å¾…è¿æ¥å»ºç«‹
            await asyncio.sleep(0.5)
            
            if not callback.is_open:
                raise Exception("ASRè¿æ¥å»ºç«‹å¤±è´¥")
            
            # åˆ†å—å‘é€éŸ³é¢‘æ•°æ®
            chunk_size = 3200  # 200ms @ 16kHz
            for i in range(0, len(audio_data), chunk_size):
                chunk = audio_data[i:i + chunk_size]
                recognition.send_audio_frame(chunk)
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå®æ—¶æµ
            
            # åœæ­¢è¯†åˆ«
            recognition.stop()
            
            # åˆå¹¶ç»“æœ
            full_text = " ".join(results)
            logger.info(f"âœ… æ–‡ä»¶è¯†åˆ«å®Œæˆ: {len(full_text)} å­—ç¬¦")
            return full_text
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            raise

# å…¨å±€æœåŠ¡å®ä¾‹
qwen_asr_realtime = QwenASRRealtimeService() 