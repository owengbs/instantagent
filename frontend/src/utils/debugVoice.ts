// ç®€å•çš„è¯­éŸ³åŠŸèƒ½è°ƒè¯•å·¥å…·

export async function quickVoiceCheck() {
  console.log('ğŸ” å¿«é€Ÿè¯­éŸ³åŠŸèƒ½æ£€æŸ¥')
  
  // 1. æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
  const hasRecognition = !!(window.SpeechRecognition || (window as any).webkitSpeechRecognition)
  const hasSynthesis = !!(window.speechSynthesis && window.SpeechSynthesisUtterance)
  const hasMediaDevices = !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia)
  
  console.log('ğŸ“Š åŸºç¡€æ”¯æŒæ£€æŸ¥:')
  console.log('  - è¯­éŸ³è¯†åˆ«API:', hasRecognition ? 'âœ…' : 'âŒ')
  console.log('  - è¯­éŸ³åˆæˆAPI:', hasSynthesis ? 'âœ…' : 'âŒ')  
  console.log('  - åª’ä½“è®¾å¤‡API:', hasMediaDevices ? 'âœ…' : 'âŒ')
  console.log('  - æµè§ˆå™¨:', navigator.userAgent)
  console.log('  - åè®®:', location.protocol)
  console.log('  - åŸŸå:', location.hostname)
  
  // 2. æ£€æŸ¥éº¦å…‹é£æƒé™
  if (hasMediaDevices) {
    try {
      console.log('ğŸ¤ æ£€æŸ¥éº¦å…‹é£æƒé™...')
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      console.log('âœ… éº¦å…‹é£æƒé™è·å–æˆåŠŸ')
      
      // ç«‹å³å…³é—­æµ
      stream.getTracks().forEach(track => track.stop())
      
      // 3. æµ‹è¯•è¯­éŸ³è¯†åˆ«åˆ›å»º
      if (hasRecognition) {
        try {
          const SpeechRecognition = window.SpeechRecognition || (window as any).webkitSpeechRecognition
          const recognition = new SpeechRecognition()
          console.log('âœ… è¯­éŸ³è¯†åˆ«å¯¹è±¡åˆ›å»ºæˆåŠŸ')
          
          recognition.lang = 'zh-CN'
          recognition.continuous = false
          recognition.interimResults = true
          
          console.log('ğŸ“ è¯­éŸ³è¯†åˆ«é…ç½®:')
          console.log('  - è¯­è¨€:', recognition.lang)
          console.log('  - è¿ç»­æ¨¡å¼:', recognition.continuous)
          console.log('  - ä¸­é—´ç»“æœ:', recognition.interimResults)
          
        } catch (recError) {
          console.error('âŒ åˆ›å»ºè¯­éŸ³è¯†åˆ«å¯¹è±¡å¤±è´¥:', recError)
        }
      }
      
    } catch (micError: any) {
      console.error('âŒ éº¦å…‹é£æƒé™è·å–å¤±è´¥:', micError)
      
      if (micError.name === 'NotAllowedError') {
        console.log('ğŸ’¡ å»ºè®®: è¯·åœ¨æµè§ˆå™¨åœ°å€æ å·¦ä¾§ç‚¹å‡»ğŸ”’å›¾æ ‡ï¼Œå…è®¸éº¦å…‹é£è®¿é—®')
      } else if (micError.name === 'NotFoundError') {
        console.log('ğŸ’¡ å»ºè®®: è¯·æ£€æŸ¥éº¦å…‹é£è®¾å¤‡æ˜¯å¦è¿æ¥')
      }
    }
  }
  
  // 4. å¿«é€Ÿè¯­éŸ³åˆæˆæµ‹è¯•
  if (hasSynthesis) {
    try {
      console.log('ğŸ—£ï¸ æµ‹è¯•è¯­éŸ³åˆæˆ...')
      const utterance = new SpeechSynthesisUtterance('')
      utterance.volume = 0
      utterance.lang = 'zh-CN'
      
      const voices = speechSynthesis.getVoices()
      console.log(`âœ… å‘ç° ${voices.length} ä¸ªå¯ç”¨è¯­éŸ³`)
      
      const chineseVoices = voices.filter(v => v.lang.startsWith('zh'))
      console.log(`ğŸ“¢ ä¸­æ–‡è¯­éŸ³æ•°é‡: ${chineseVoices.length}`)
      
      if (chineseVoices.length > 0) {
        console.log('ğŸ¯ æ¨èä¸­æ–‡è¯­éŸ³:', chineseVoices[0].name)
      }
      
    } catch (synthError) {
      console.error('âŒ è¯­éŸ³åˆæˆæµ‹è¯•å¤±è´¥:', synthError)
    }
  }
  
  console.log('\nğŸ¯ æ£€æŸ¥å®Œæˆ! å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®ã€‚')
}

// è‡ªåŠ¨æŒ‚è½½åˆ°å…¨å±€
if (typeof window !== 'undefined') {
  (window as any).quickVoiceCheck = quickVoiceCheck
} 