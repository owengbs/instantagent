import { useState, useRef, useCallback, useEffect } from 'react'

interface QwenSpeechRecognitionOptions {
  language?: string
  model?: string
  onStart?: () => void
  onEnd?: () => void
  onResult?: (text: string, isFinal: boolean) => void
  onError?: (error: string) => void
}

interface QwenSpeechRecognitionReturn {
  isSupported: boolean
  isListening: boolean
  isConnecting: boolean
  transcript: string
  finalTranscript: string
  error: string | null
  startListening: () => Promise<void>
  stopListening: () => void
  resetTranscript: () => void
}

const API_BASE_URL = 'ws://localhost:8000/api/asr/ws'

// Web Audio API Èü≥È¢ëÈááÈõÜÂô®
class AudioRecorder {
  private context: AudioContext
  private stream: MediaStream | null = null
  private processor: ScriptProcessorNode | null = null
  private isRecording = false
  private sampleRate: number
  private onAudioData: (data: Int16Array) => void

  constructor(sampleRate: number = 16000, onAudioData: (data: Int16Array) => void) {
    this.context = new (window.AudioContext || (window as any).webkitAudioContext)()
    this.sampleRate = sampleRate
    this.onAudioData = onAudioData
  }

  async start(): Promise<void> {
    try {
      // Ëé∑ÂèñÈ∫¶ÂÖãÈ£éÊùÉÈôê
      this.stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: this.sampleRate,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      // ÂàõÂª∫Èü≥È¢ëÊ∫ê
      const source = this.context.createMediaStreamSource(this.stream)
      
      // ÂàõÂª∫Èü≥È¢ëÂ§ÑÁêÜÂô®
      const bufferSize = 4096
      this.processor = this.context.createScriptProcessor(bufferSize, 1, 1)
      
      // Â§ÑÁêÜÈü≥È¢ëÊï∞ÊçÆ
      this.processor.onaudioprocess = (event) => {
        if (!this.isRecording) return
        
        const inputBuffer = event.inputBuffer
        const inputData = inputBuffer.getChannelData(0)
        
        // ËΩ¨Êç¢‰∏∫Int16Array (PCMÊ†ºÂºè)
        const pcmData = new Int16Array(inputData.length)
        for (let i = 0; i < inputData.length; i++) {
          pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768))
        }
        
        // ÂõûË∞ÉÈü≥È¢ëÊï∞ÊçÆ
        this.onAudioData(pcmData)
      }
      
      // ËøûÊé•Èü≥È¢ëËäÇÁÇπ
      source.connect(this.processor)
      this.processor.connect(this.context.destination)
      
      this.isRecording = true
      
    } catch (error) {
      throw new Error(`ÂêØÂä®Èü≥È¢ëÂΩïÂà∂Â§±Ë¥•: ${error}`)
    }
  }

  stop(): void {
    this.isRecording = false
    
    if (this.processor) {
      this.processor.disconnect()
      this.processor = null
    }
    
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop())
      this.stream = null
    }
  }

  getIsRecording(): boolean {
    return this.isRecording
  }
}

export const useQwenSpeechRecognition = (options: QwenSpeechRecognitionOptions = {}): QwenSpeechRecognitionReturn => {
  const {
    language = 'zh-CN',
    model = 'paraformer-realtime-v2',
    onStart,
    onEnd,
    onResult,
    onError
  } = options

  const [isSupported] = useState(true) // ÂÅáËÆæÊµèËßàÂô®ÊîØÊåÅWeb Audio API
  const [isListening, setIsListening] = useState(false)
  const [isConnecting, setIsConnecting] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [finalTranscript, setFinalTranscript] = useState('')
  const [error, setError] = useState<string | null>(null)

  const recorderRef = useRef<AudioRecorder | null>(null)
  const wsRef = useRef<WebSocket | null>(null)
  const reconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null)

  // Èü≥È¢ëÊï∞ÊçÆÂõûË∞É
  const handleAudioData = useCallback((pcmData: Int16Array) => {
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      // ÂèëÈÄÅÈü≥È¢ëÊï∞ÊçÆÂà∞WebSocket
      wsRef.current.send(pcmData.buffer)
    }
  }, [])

  // ËøûÊé•WebSocket
  const connectWebSocket = useCallback(async () => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      return
    }

    console.log('üîå ËøûÊé•Qwen ASR WebSocket...')
    setError(null)
    setIsConnecting(true)

    try {
      const ws = new WebSocket(`${API_BASE_URL}/recognize`)
      wsRef.current = ws

      ws.onopen = () => {
        console.log('‚úÖ Qwen ASR WebSocketËøûÊé•ÊàêÂäü')
        setIsConnecting(false)
        setError(null)
        
        // ÂèëÈÄÅËØÜÂà´ÂºÄÂßã‰∫ã‰ª∂
        ws.send(JSON.stringify({
          type: 'start',
          model: model,
          language: language
        }))
      }

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data)
          const { type, text, is_final } = data

          switch (type) {
            case 'partial':
              // ÈÉ®ÂàÜËØÜÂà´ÁªìÊûú
              if (text && text.trim()) {
                setTranscript(text)
                onResult?.(text, false)
              }
              break

            case 'sentence':
              // Âè•Â≠êÁ∫ßÂà´ËØÜÂà´ÁªìÊûú
              if (text && text.trim()) {
                setFinalTranscript(prev => prev + text)
                setTranscript('') // Ê∏ÖÁ©∫ÈÉ®ÂàÜÁªìÊûú
                onResult?.(text, is_final || false)
              }
              break

            case 'error':
              const errorMsg = data.message || 'ËØ≠Èü≥ËØÜÂà´ÈîôËØØ'
              console.error('‚ùå ASRÈîôËØØ:', errorMsg)
              setError(errorMsg)
              onError?.(errorMsg)
              break

            case 'end':
              console.log('‚úÖ ASRËØÜÂà´ÁªìÊùü')
              break
          }
        } catch (err) {
          console.error('‚ùå Â§ÑÁêÜASRÊ∂àÊÅØÂ§±Ë¥•:', err)
        }
      }

      ws.onclose = (event) => {
        console.log('üîå Qwen ASR WebSocketËøûÊé•ÂÖ≥Èó≠:', event.code, event.reason)
        setIsConnecting(false)
        setIsListening(false)
        
        // Ëá™Âä®ÈáçËøûÔºàÈô§ÈùûÊòØÊ≠£Â∏∏ÂÖ≥Èó≠Ôºâ
        if (event.code !== 1000) {
          scheduleReconnect()
        }
      }

      ws.onerror = (error) => {
        console.error('‚ùå Qwen ASR WebSocketÈîôËØØ:', error)
        setError('WebSocketËøûÊé•ÈîôËØØ')
        setIsConnecting(false)
      }

    } catch (err) {
      console.error('‚ùå ÂàõÂª∫ASR WebSocketËøûÊé•Â§±Ë¥•:', err)
      setError('Êó†Ê≥ïÂª∫Á´ãËøûÊé•')
      setIsConnecting(false)
    }
  }, [language, model, onResult, onError])

  // ÈáçËøûË∞ÉÂ∫¶
  const scheduleReconnect = useCallback(() => {
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current)
    }
    
    reconnectTimeoutRef.current = setTimeout(() => {
      console.log('üîÑ Â∞ùËØïÈáçËøûQwen ASR WebSocket...')
      connectWebSocket()
    }, 3000)
  }, [connectWebSocket])

  // ÂºÄÂßãÁõëÂê¨
  const startListening = useCallback(async () => {
    try {
      console.log('üé§ ÂºÄÂßãQwenËØ≠Èü≥ËØÜÂà´...')
      setError(null)
      onStart?.()

      // ËøûÊé•WebSocket
      await connectWebSocket()

      // ÂàõÂª∫Èü≥È¢ëÂΩïÂà∂Âô®
      recorderRef.current = new AudioRecorder(16000, handleAudioData)
      
      // ÂºÄÂßãÂΩïÂà∂
      await recorderRef.current.start()
      setIsListening(true)
      
      console.log('‚úÖ QwenËØ≠Èü≥ËØÜÂà´Â∑≤ÂêØÂä®')

    } catch (err) {
      console.error('‚ùå ÂêØÂä®QwenËØ≠Èü≥ËØÜÂà´Â§±Ë¥•:', err)
      const errorMessage = err instanceof Error ? err.message : String(err)
      setError(errorMessage)
      onError?.(errorMessage)
    }
  }, [connectWebSocket, handleAudioData, onStart, onError])

  // ÂÅúÊ≠¢ÁõëÂê¨
  const stopListening = useCallback(() => {
    console.log('üõë ÂÅúÊ≠¢QwenËØ≠Èü≥ËØÜÂà´')
    
    // ÂÅúÊ≠¢Èü≥È¢ëÂΩïÂà∂
    if (recorderRef.current) {
      recorderRef.current.stop()
      recorderRef.current = null
    }

    // ÂèëÈÄÅÁªìÊùü‰ø°Âè∑
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify({ type: 'end' }))
    }

    setIsListening(false)
    onEnd?.()
  }, [onEnd])

  // ÈáçÁΩÆËΩ¨ÂΩï
  const resetTranscript = useCallback(() => {
    setTranscript('')
    setFinalTranscript('')
  }, [])

  // ÁªÑ‰ª∂Âç∏ËΩΩÊó∂Ê∏ÖÁêÜ
  useEffect(() => {
    return () => {
      if (reconnectTimeoutRef.current) {
        clearTimeout(reconnectTimeoutRef.current)
      }
      
      if (wsRef.current) {
        wsRef.current.close()
      }
      
      if (recorderRef.current) {
        recorderRef.current.stop()
      }
    }
  }, [])

  return {
    isSupported,
    isListening,
    isConnecting,
    transcript,
    finalTranscript,
    error,
    startListening,
    stopListening,
    resetTranscript
  }
} 