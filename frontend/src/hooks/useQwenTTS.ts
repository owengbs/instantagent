import { useState, useEffect, useRef, useCallback } from 'react'

interface QwenTTSVoice {
  name: string
  language: string
  description: string
  dialect: string
}

interface QwenTTSOptions {
  voice?: string
  onStart?: () => void
  onEnd?: () => void
  onError?: (error: string) => void
}

interface QwenTTSReturn {
  isSupported: boolean
  isSpeaking: boolean
  speak: (text: string) => Promise<void>
  stop: () => void
  voices: QwenTTSVoice[]
  error: string | null
  isLoading: boolean
}

import { API_CONFIG } from '../config/api'

const API_BASE_URL = API_CONFIG.endpoints.tts()

// åœ¨å¼€å‘/å¤–ç½‘è°ƒè¯•ï¼ˆå¦‚ ngrokï¼‰æ—¶ï¼Œä¼˜å…ˆèµ°ç›¸å¯¹è·¯å¾„å‘½ä¸­ Vite ä»£ç†ï¼Œé¿å…è·¨åŸŸ/è¯ä¹¦ç­‰å¯¼è‡´è¿”å›HTML
const buildUrl = (path: string) => {
  const base = API_BASE_URL
  if (!base) return `/api/tts${path}`
  // base å¯èƒ½ä¸º ''ï¼ˆç›¸å¯¹ï¼‰ï¼Œæˆ– 'http(s)://.../api/tts'
  return base.startsWith('http') ? `${base}${path}` : `/api/tts${path}`
}

// Web Audio API ä¸Šä¸‹æ–‡
let audioContext: AudioContext | null = null

// è·å–æˆ–åˆ›å»ºAudioContext
const getAudioContext = (): AudioContext => {
  if (!audioContext) {
    audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
  }
  return audioContext
}

export const useQwenTTS = (options: QwenTTSOptions = {}): QwenTTSReturn => {
  const {
    voice = 'Cherry',
    onStart,
    onEnd,
    onError
  } = options

  const [isSupported, setIsSupported] = useState(true) // Qwen-TTS always supported
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [voices, setVoices] = useState<QwenTTSVoice[]>([])
  const [error, setError] = useState<string | null>(null)
  const [isLoading, setIsLoading] = useState(false)

  const currentAudioRef = useRef<any>(null)  // æ”¯æŒHTMLAudioElementå’ŒWeb Audio APIå¯¹è±¡
  const abortControllerRef = useRef<AbortController | null>(null)

  // è·å–å¯ç”¨è¯­éŸ³åˆ—è¡¨ - ç§»é™¤ä¾èµ–é¿å…æ— é™å¾ªç¯
  const fetchVoices = useCallback(async () => {
    try {
      console.log('ğŸ¤ è·å–Qwen-TTSè¯­éŸ³åˆ—è¡¨...')
      const response = await fetch(buildUrl('/voices'))
      
      if (!response.ok) {
        throw new Error(`è·å–è¯­éŸ³åˆ—è¡¨å¤±è´¥: ${response.status}`)
      }

      const voicesData = await response.json()
      console.log('ğŸµ è·å–åˆ°è¯­éŸ³åˆ—è¡¨:', voicesData)
      setVoices(voicesData)
      
      // æ·»åŠ é»˜è®¤è¯­éŸ³é€‰é¡¹ï¼ˆé˜²æ­¢APIè¿”å›ç©ºæ•°ç»„ï¼‰
      if (voicesData.length === 0) {
        setVoices([
          { name: 'Cherry', language: 'Chinese/English', description: 'æ¸©æŸ”ç”œç¾å¥³å£°', dialect: 'Standard Mandarin' },
          { name: 'Ethan', language: 'Chinese/English', description: 'æˆç†Ÿç¨³é‡ç”·å£°', dialect: 'Standard Mandarin' },
          { name: 'Chelsie', language: 'Chinese/English', description: 'æ´»æ³¼å¯çˆ±å¥³å£°', dialect: 'Standard Mandarin' },
          { name: 'Serena', language: 'Chinese/English', description: 'ä¼˜é›…çŸ¥æ€§å¥³å£°', dialect: 'Standard Mandarin' }
        ])
      }
    } catch (err) {
      console.error('âŒ è·å–è¯­éŸ³åˆ—è¡¨å¤±è´¥:', err)
      setError('è·å–è¯­éŸ³åˆ—è¡¨å¤±è´¥')
      
      // è®¾ç½®é»˜è®¤è¯­éŸ³
      setVoices([
        { name: 'Cherry', language: 'Chinese/English', description: 'æ¸©æŸ”ç”œç¾å¥³å£°', dialect: 'Standard Mandarin' }
      ])
    }
  }, []) // ç§»é™¤onErrorä¾èµ–

  // æ£€æŸ¥TTSæœåŠ¡å¥åº·çŠ¶æ€ - ç§»é™¤ä¾èµ–é¿å…æ— é™å¾ªç¯
  const checkTTSHealth = useCallback(async () => {
    try {
      const response = await fetch(buildUrl('/health'))
      const health = await response.json()
      
      if (health.status !== 'healthy') {
        console.warn('âš ï¸ TTSæœåŠ¡çŠ¶æ€å¼‚å¸¸:', health)
        setError('TTSæœåŠ¡ä¸å¯ç”¨')
        setIsSupported(false)
      } else {
        console.log('âœ… TTSæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡')
        setIsSupported(true)
        setError(null)
      }
    } catch (err) {
      console.error('âŒ TTSå¥åº·æ£€æŸ¥å¤±è´¥:', err)
      setError('TTSæœåŠ¡è¿æ¥å¤±è´¥')
      setIsSupported(false)
    }
  }, []) // ä¿æŒç©ºä¾èµ–æ•°ç»„

  // è¯­éŸ³åˆæˆå‡½æ•°
  const speak = useCallback(async (text: string) => {
    if (!text.trim()) {
      console.warn('âš ï¸ æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡è¯­éŸ³åˆæˆ')
      return
    }

    if (isSpeaking) {
      console.log('ğŸ›‘ æ­£åœ¨æ’­æ”¾ä¸­ï¼Œå…ˆåœæ­¢å½“å‰æ’­æ”¾')
      stop()
    }

    try {
      console.log('ğŸ—£ï¸ å¼€å§‹Qwen-TTSè¯­éŸ³åˆæˆ:', { text: text.slice(0, 50) + '...', voice })
      
      setIsSpeaking(true)
      setIsLoading(true)
      setError(null)
      onStart?.()

      // åˆ›å»ºä¸­æ­¢æ§åˆ¶å™¨
      abortControllerRef.current = new AbortController()

      // è°ƒç”¨TTS API
      const response = await fetch(buildUrl('/synthesize'), {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          text: text.trim(),
          voice: voice
        }),
        signal: abortControllerRef.current.signal
      })

      if (!response.ok) {
        const errorText = await response.text()
        throw new Error(`TTSåˆæˆå¤±è´¥: ${response.status} - ${errorText}`)
      }

      // è·å–éŸ³é¢‘æ•°æ® (PCMæ ¼å¼)
      const arrayBuffer = await response.arrayBuffer()
      const sampleRate = parseInt(response.headers.get('X-Sample-Rate') || '24000')
      const channels = parseInt(response.headers.get('X-Channels') || '1')
      
      console.log('ğŸµ PCMéŸ³é¢‘åˆæˆå®Œæˆï¼Œå¼€å§‹æ’­æ”¾', { 
        size: arrayBuffer.byteLength, 
        sampleRate, 
        channels 
      })
      setIsLoading(false)

             // ä½¿ç”¨Web Audio APIæ’­æ”¾PCMéŸ³é¢‘
       await playPCMAudio(arrayBuffer, sampleRate, channels)

       // PCMéŸ³é¢‘æ’­æ”¾å‡½æ•°
       async function playPCMAudio(arrayBuffer: ArrayBuffer, sampleRate: number, channels: number) {
         try {
           const context = getAudioContext()
           
           // ç¡®ä¿AudioContextå·²æ¢å¤
           if (context.state === 'suspended') {
             await context.resume()
           }
           
           // å°†PCMæ•°æ®è½¬æ¢ä¸ºFloat32Array
           const pcmData = new Int16Array(arrayBuffer)
           const audioBuffer = context.createBuffer(channels, pcmData.length / channels, sampleRate)
           
           // è½¬æ¢16ä½PCMåˆ°Float32æ ¼å¼
           const channelData = audioBuffer.getChannelData(0)
           for (let i = 0; i < pcmData.length; i++) {
             channelData[i] = pcmData[i] / 32768.0  // 16ä½PCMå½’ä¸€åŒ–
           }
           
           // åˆ›å»ºéŸ³é¢‘æº
           const source = context.createBufferSource()
           source.buffer = audioBuffer
           source.connect(context.destination)
           
           // è®¾ç½®æ’­æ”¾äº‹ä»¶
           source.onended = () => {
             console.log('âœ… PCMéŸ³é¢‘æ’­æ”¾ç»“æŸ')
             setIsSpeaking(false)
             setIsLoading(false)
             currentAudioRef.current = null
             onEnd?.()
           }
           
           // ä¿å­˜å¼•ç”¨ä»¥ä¾¿åœæ­¢
           currentAudioRef.current = { 
             stop: () => source.stop(),
             pause: () => source.stop(), // PCMéŸ³é¢‘æ— æ³•æš‚åœï¼Œåªèƒ½åœæ­¢
             currentTime: 0
           } as any
           
           // å¼€å§‹æ’­æ”¾
           console.log('â–¶ï¸ PCMéŸ³é¢‘æ’­æ”¾å¼€å§‹')
           setIsSpeaking(true)
           source.start(0)
           onStart?.()
           
         } catch (err) {
           console.error('âŒ PCMéŸ³é¢‘æ’­æ”¾é”™è¯¯:', err)
           setIsSpeaking(false)
           setIsLoading(false)
           setError('PCMéŸ³é¢‘æ’­æ”¾å¤±è´¥')
           currentAudioRef.current = null
           onError?.('PCMéŸ³é¢‘æ’­æ”¾å¤±è´¥')
         }
       }

    } catch (err) {
      console.error('âŒ Qwen-TTSè¯­éŸ³åˆæˆå¤±è´¥:', err)
      setIsSpeaking(false)
      setIsLoading(false)
      
      if (err instanceof Error && err.name === 'AbortError') {
        console.log('ğŸ›‘ è¯­éŸ³åˆæˆè¢«ç”¨æˆ·å–æ¶ˆ')
        return
      }
      
      const errorMessage = err instanceof Error ? err.message : String(err)
      setError(errorMessage)
    }
  }, [voice]) // åªä¿ç•™voiceä¾èµ–ï¼Œé¿å…æ— é™å¾ªç¯

  // åœæ­¢æ’­æ”¾
  const stop = useCallback(() => {
    console.log('ğŸ›‘ åœæ­¢Qwen-TTSæ’­æ”¾')
    
    // ä¸­æ­¢ç½‘ç»œè¯·æ±‚
    if (abortControllerRef.current) {
      abortControllerRef.current.abort()
      abortControllerRef.current = null
    }

    // åœæ­¢éŸ³é¢‘æ’­æ”¾ (æ”¯æŒWeb Audio APIå’Œä¼ ç»ŸAudio)
    if (currentAudioRef.current) {
      try {
        if (typeof currentAudioRef.current.stop === 'function') {
          // Web Audio API AudioBufferSourceNode
          currentAudioRef.current.stop()
        } else if (typeof currentAudioRef.current.pause === 'function') {
          // ä¼ ç»ŸHTML Audioå…ƒç´ 
          currentAudioRef.current.pause()
          currentAudioRef.current.currentTime = 0
        }
      } catch (err) {
        console.warn('åœæ­¢éŸ³é¢‘æ—¶å‡ºç°è­¦å‘Š:', err)
      }
      currentAudioRef.current = null
    }

    setIsSpeaking(false)
    setIsLoading(false)
  }, [])

  // ç»„ä»¶åˆå§‹åŒ– - åªæ‰§è¡Œä¸€æ¬¡
  useEffect(() => {
    console.log('ğŸ”§ åˆå§‹åŒ–Qwen-TTS Hook')
    checkTTSHealth()
    fetchVoices()
    
    // æ¸…ç†å‡½æ•°
    return () => {
      stop()
    }
  }, []) // ç©ºä¾èµ–æ•°ç»„ï¼Œåªåœ¨ç»„ä»¶æŒ‚è½½æ—¶æ‰§è¡Œä¸€æ¬¡

  return {
    isSupported,
    isSpeaking,
    speak,
    stop,
    voices,
    error,
    isLoading
  }
} 